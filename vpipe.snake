import csv
import collections
import configparser
import os
import sys
import typing

# 1. parse config file
#
# the config object sets defaults for a number of parameters and
# validates parameters given by the user to be sensible

VPIPE_DEBUG = True if os.environ.get('VPIPE_DEBUG') is not None else False


class VpipeConfig(object):
    'Class used to encapsulate the configuration properties used by V-pipe'

    __RECORD__ = typing.NamedTuple(
        "__RECORD__", [('value', typing.Any), ('type', type)])
    __MEMBER_DEFAULT__ = collections.OrderedDict([
        ('general', {
            'threads': __RECORD__(value=4, type=int),
        }),
        ('input', {
            'datadir': __RECORD__(value='samples', type=str),
            'samples_file': __RECORD__(value='samples.tsv', type=str),
            'fastq_suffix': __RECORD__(value='', type=str),
            'trim_cutoff': __RECORD__(value=200, type=float),
            'reference': __RECORD__(value='references/HXB2.fasta', type=str),
        }),
        ('output', {
            'QA': __RECORD__(value=False, type=bool),
            'snv': __RECORD__(value=True, type=bool),
            'local': __RECORD__(value=True, type=bool),
            'global': __RECORD__(value=True, type=bool),
        }),
        ('applications', {
            'gunzip': __RECORD__(value="gunzip", type=str),
            'prinseq': __RECORD__(value="prinseq-lite.pl", type=str),
            'vicuna': __RECORD__(value="vicuna", type=str),
            'bwa': __RECORD__(value="bwa", type=str),
            'samtools': __RECORD__(value="samtools", type=str),
            'mafft': __RECORD__(value="mafft", type=str),
            'ngshmmalign': __RECORD__(value="ngshmmalign", type=str),
            'convert_reference': __RECORD__(value="convert_reference", type=str),
            'extract_seq': __RECORD__(value="extract_seq", type=str),
            'coverage_stats': __RECORD__(value="coverage_stats", type=str),
            'remove_gaps_msa': __RECORD__(value="remove_gaps_msa", type=str),
            'minority_freq': __RECORD__(value="minority_freq", type=str),
            'extract_coverage_intervals': __RECORD__(value="extract_coverage_intervals", type=str),
            'shorah': __RECORD__(value="dec.py", type='str'),
            'haploclique': __RECORD__(value="haploclique", type='str'),
            'compute_mds': __RECORD__(value="compute_mds", type='str')
        }),

        ('gunzip', {
            'mem': __RECORD__(value=30000, type=int),
            'time': __RECORD__(value=60, type=int),
        }),
        ('extract', {
            'mem': __RECORD__(value=10000, type=int),
            'time': __RECORD__(value=20, type=int),
        }),
        ('preprocessing', {
            'mem': __RECORD__(value=2000, type=int),
            'time': __RECORD__(value=235, type=int),
            'conda': __RECORD__(value='', type=str),

            'qual_threshold': __RECORD__(value=30, type=int),
            'min_len': __RECORD__(value=0.8, type=float),
        }),
        ('initial_vicuna', {
            'mem': __RECORD__(value=1000, type=int),
            'time': __RECORD__(value=600, type=int),
            'threads': __RECORD__(value=0, type=int),
            'conda': __RECORD__(value='', type=str),
        }),
        ('initial_vicuna_msa', {
            'mem': __RECORD__(value=10000, type=int),
            'time': __RECORD__(value=235, type=int),
            'threads': __RECORD__(value=0, type=int),
            'conda': __RECORD__(value='', type=str),
        }),
        ('create_vicuna_initial', {
            'conda': __RECORD__(value='envs/smallgenomeutilities.yaml', type=str),
        }),
        ('hmm_align', {
            'mem': __RECORD__(value=1250, type=int),
            'time': __RECORD__(value=1435, type=int),
            'threads': __RECORD__(value=0, type=int),
            'conda': __RECORD__(value='', type=str),

            'leave_msa_temp': __RECORD__(value=False, type=bool),
        }),
        ('sam2bam', {
            'mem': __RECORD__(value=5000, type=int),
            'time': __RECORD__(value=30, type=int),
            'conda': __RECORD__(value='', type=str),
        }),
        ('bwa_align', {
            'mem': __RECORD__(value=1250, type=int),
            'time': __RECORD__(value=235, type=int),
            'threads': __RECORD__(value=0, type=int),
            'conda': __RECORD__(value='', type=str),
        }),
        ('coverage_QA', {
            'mem': __RECORD__(value=1250, type=int),
            'time': __RECORD__(value=235, type=int),
            'conda': __RECORD__(value='envs/smallgenomeutilities.yaml', type=str),
        }),
        ('msa', {
            'mem': __RECORD__(value=10000, type=int),
            'time': __RECORD__(value=235, type=int),
            'threads': __RECORD__(value=0, type=int),
            'conda': __RECORD__(value='', type=str),
        }),
        ('convert_to_ref', {
            'mem': __RECORD__(value=8000, type=int),
            'time': __RECORD__(value=235, type=int),
            'conda': __RECORD__(value='envs/smallgenomeutilities.yaml', type=str),
        }),
        ('minor_variants', {
            'mem': __RECORD__(value=1000, type=int),
            'time': __RECORD__(value=235, type=int),
            'threads': __RECORD__(value=0, type=int),
            'conda': __RECORD__(value='envs/smallgenomeutilities.yaml', type=str),
        }),
        ('coverage_intervals', {
            'mem': __RECORD__(value=1000, type=int),
            'time': __RECORD__(value=60, type=int),

            'coverage': __RECORD__(value=50, type=int),
            'liberal': __RECORD__(value=True, type=bool),
        }),
        ('snv', {
            'mem': __RECORD__(value=10000, type=int),
            'time': __RECORD__(value=2880, type=int),
            'threads': __RECORD__(value=0, type=int),
            'conda': __RECORD__(value='', type=str),

            'shift': __RECORD__(value=3, type=int),
            'keep_files': __RECORD__(value=False, type=bool),
        }),
        ('aggregate', {
            'mem': __RECORD__(value=2000, type=int),
            'time': __RECORD__(value=235, type=int),
        }),
        ('haploclique', {
            'mem': __RECORD__(value=10000, type=int),
            'time': __RECORD__(value=1435, type=int),

            'relax': __RECORD__(value=True, type=bool),
            'no_singletons': __RECORD__(value=True, type=bool),
            'no_prob0': __RECORD__(value=True, type=bool),
            'clique_size_limit': __RECORD__(value=3, type=int),
            'max_num_cliques': __RECORD__(value=10000, type=int),
        }),
        ('haploclique_visualization', {
            'mem': __RECORD__(value=2000, type=int),
            'time': __RECORD__(value=235, type=int),
            'conda': __RECORD__(value='envs/smallgenomeutilities.yaml', type=str),

            'region_start': __RECORD__(value=0, type=int),
            'region_end': __RECORD__(value=9719, type=int),
            'msa': __RECORD__(value='', type=str),
        })
    ])

    def __init__(self):
        self.__members = {}

        vpipe_configfile = configparser.ConfigParser()
        vpipe_configfile.read('vpipe.config')

        for (section, properties) in VpipeConfig.__MEMBER_DEFAULT__.items():
            self.__members[section] = {}

            for (value, defaults) in properties.items():
                try:
                    if defaults.type == int:
                        cur_value = vpipe_configfile.getint(section, value)
                    elif defaults.type == float:
                        cur_value = vpipe_configfile.getfloat(section, value)
                    elif defaults.type == bool:
                        cur_value = vpipe_configfile.getboolean(section, value)
                    else:
                        cur_value = vpipe_configfile.get(section, value)
                    state = 'user'
                except (configparser.NoSectionError, configparser.NoOptionError):
                    if value == 'threads' and section != 'general':
                        cur_value = defaults.value if defaults.value else self.__members[
                            'general']['threads']
                    elif value == 'conda':
                        cur_value = "envs/{}.yaml".format(section) if len(
                            defaults.value) == 0 else defaults.value
                    else:
                        cur_value = defaults.value
                    state = 'DEFAULT'
                except ValueError:
                    print("ERROR: Property '{}' of section '{}' has to be of type '{}', whereas you gave '{}'!".format(
                        value, section, defaults.type.__name__, vpipe_configfile[section][value]))
                    raise

                if VPIPE_DEBUG:
                    print("Using {} value '{}' for property '{}' in section '{}'".format(
                        state, cur_value, value, section))

                self.__members[section][value] = cur_value

    def __getattr__(self, name):
        try:
            return self.__members[name]
        except:
            print("ERROR: Section '{}' is not a valid section!".format(name))
            raise


config = VpipeConfig()


# 2. glob patients/samples + store as TSV if file is not provided

# if file containing samples exists, proceed
# to build list of target files
if not os.path.isfile(config.input['samples_file']):
    # sample file does not exist, have to first glob
    # all patients' data and then construct sample
    # list that would pass QA checks

    patient_sample_pairs = glob_wildcards(
        "{}/{{patient_date}}/raw_data/{{file}}_R1{}.fastq.gz".format(config.input['datadir'], config.input['fastq_suffix']))

    with open('samples.tsv', 'w') as outfile:
        for i in patient_sample_pairs.patient_date:
            (patient, date) = [x.strip() for x in i.split("/") if x.strip()]
            outfile.write('{}\t{}\n'.format(patient, date))

    # TODO: have to preprocess patient files to filter likely failures
    # 1.) Determine 5%/95% length of FASTQ files
    # 2.) Determine whether FASTQ would survive


# 3. load patients from TSV and create list of samples
#
# This list is reused on subsequent runs

patient_list = []
patient_dict = {}
patient_record = typing.NamedTuple(
    "patient_record", [('patient_id', str), ('date', str)])

with open(config.input['samples_file'], newline='') as csvfile:
    spamreader = csv.reader(csvfile, delimiter='\t')

    for row in spamreader:
        assert len(row) >= 2, "ERROR: Line '{}' does not contain at least two entries!".format(
                              spamreader.line_num)
        patient_tuple = patient_record(patient_id=row[0], date=row[1])
        patient_list.append(patient_tuple)

        if len(row) == 2:
            # All samples are assumed to have same read length and 'trim_cutoff'
            # must correspond to the threshold value in the length of the reads
            # after quality trimming
            assert config.input['trim_cutoff'] > 1, "ERROR: 'trim_cutoff' is expected to be larger than 1, whereas 'trim_cutoff'={}".format(
                config.input['trim_cutoff'])

            assert patient_tuple not in patient_dict, "ERROR: sample '{}-{}' is not unique".format(
                                                      row[0], row[1])
            patient_dict[patient_tuple] = int(config.input['trim_cutoff'])

        elif len(row) == 3:
            assert config.input['trim_cutoff'] < 0 or config.input['trim_cutoff'] > 1, "ERROR: 'trim_cutoff' is expected to be a fraction (between 0 and 1), whereas 'trim_cutoff'={}".format(
                config.input['trim_cutoff'])
            # Extract read length from input.samples_file. Samples may have
            # different read lengths. Reads will be filtered out if read length
            # after trimming is less than trim_cutoff * read_length. If snvs
            # are expected as output, third column should be provided
            assert patient_list not in patient_dict, "ERROR: sample '{}-{}' is not unique".format(
                row[0], row[1])
            patient_dict[patient_tuple] = int(
                config.input['trim_cutoff'] * row[3])

# 4. generate list of target files
all_files = []
alignments = []
vicuna_refs = []
references = []
trimmed_files = []
results = []
datasets = []
IDs = []
for p in patient_list:

    alignments.append(
        "{sample_dir}/{patient}/{date}/alignments/REF_aln.bam".format(sample_dir=config.input['datadir'], patient=p.patient_id, date=p.date))
    if config.output['QA']:
        alignments.append(
            "{sample_dir}/{patient}/{date}/QA_alignments/coverage_ambig.tsv".format(sample_dir=config.input['datadir'], patient=p.patient_id, date=p.date))
        alignments.append(
            "{sample_dir}/{patient}/{date}/QA_alignments/coverage_majority.tsv".format(sample_dir=config.input['datadir'], patient=p.patient_id, date=p.date))

    vicuna_refs.append(
        "{sample_dir}/{patient}/{date}/references/vicuna_consensus.fasta".format(sample_dir=config.input['datadir'], patient=p.patient_id, date=p.date))
    references.append(
        "{sample_dir}/{patient}/{date}/references/ref_".format(sample_dir=config.input['datadir'], patient=p.patient_id, date=p.date))

    trimmed_files.append(
        "{sample_dir}/{patient}/{date}/preprocessed_data/R1.fastq.gz".format(sample_dir=config.input['datadir'], patient=p.patient_id, date=p.date))
    trimmed_files.append(
        "{sample_dir}/{patient}/{date}/preprocessed_data/R2.fastq.gz".format(sample_dir=config.input['datadir'], patient=p.patient_id, date=p.date))

    datasets.append("{sample_dir}/{patient}/{date}".format(
        sample_dir=config.input['datadir'], patient=p.patient_id, date=p.date))
    IDs.append(('{}-{}').format(p.patient_id, p.date))

    # SNV
    if config.output['snv']:
        results.append(
            "{sample_dir}/{patient}/{date}/variants/SNVs/snvs.csv".format(sample_dir=config.input['datadir'], patient=p.patient_id, date=p.date))
    # local haplotypes
    if config.output['local']:
        results.append(
            "{sample_dir}/{patient}/{date}/variants/local/snvs.tsv".format(sample_dir=config.input['datadir'], patient=p.patient_id, date=p.date))
    # global haplotypes
    if config.output['global']:
        results.append(
            "{sample_dir}/{patient}/{date}/variants/global/quasispecies.bam".format(sample_dir=config.input['datadir'], patient=p.patient_id, date=p.date))

    # merge lists contaiing expected output
    all_files = alignments + results

IDs = ','.join(IDs)

# 5. Locate reference and parse reference identifier
try:
    if os.path.isfile(config.input['reference']):
        reference_file = config.input['reference']
    elif os.path.isfile(os.path.join("references", config.input['reference'])):
        reference_file = os.path.join("references", config.input['reference'])
except:
    print("ERROR: Reference file {} does not exists".format(
        config.input['reference']))
    raise

with open(reference_file, 'r') as infile:
    reference_name = infile.readline().rstrip()
reference_name = reference_name.split('>')[1]

# DUMMY RULES
rule all:
    input:
        all_files

rule alltrimmed:
    input:
        trimmed_files


# 1. extract
rule gunzip:
    input:
        "{file}.fastq.gz"
    output:
        temp("{file}.fastq")
    params:
        lsfoutfile = "/dev/null",
        lsferrfile = "/dev/null",
        scratch = '10000',
        mem = config.gunzip['mem'],
        time = config.gunzip['time'],
        GUNZIP = config.applications['gunzip'],
    threads:
        1
    shell:
        """
        {params.GUNZIP} -c {input} > {output}
        """


def construct_input_fastq(wildcards):
    inferred_values = glob_wildcards(
        wildcards.dataset + "/raw_data/{file}R" + wildcards.pair + config.input['fastq_suffix'] + ".fastq.gz")
    if len(inferred_values.file) == 0:
        inferred_values = glob_wildcards(
            wildcards.dataset + "/raw_data/{file}R" + wildcards.pair + config.input['fastq_suffix'] + ".fastq")

    list_output = []

    for i in inferred_values.file:
        list_output.append(wildcards.dataset + "/raw_data/" +
                           i + "R" + wildcards.pair + config.input['fastq_suffix'] + ".fastq")
    return list_output


rule extract:
    input:
        construct_input_fastq
    output:
        temp("{dataset}/extracted_data/R{pair}.fastq")
    params:
        lsfoutfile = "{dataset}/extracted_data/extract.lsfout.log",
        lsferrfile = "{dataset}/extracted_data/extract.lsferr.log",
        scratch = '2000',
        mem = config.extract['mem'],
        time = config.extract['time'],
    benchmark:
        "{dataset}/extracted_data/extract.benchmark"
    threads:
        1
    shell:
        """
        cat {input} | paste - - - - | sort -k1,1 -t " " | tr "\t" "\n" > {output}
        """

rule extractclean:
    params:
        DIR = config.input['datadir']
    shell:
        """
        rm -rf {params.DIR}/*/*/extracted_data
        """


# 2. clipping
rule preprocessing:
    input:
        R1 = "{dataset}/extracted_data/R1.fastq",
        R2 = "{dataset}/extracted_data/R2.fastq"
    output:
        R1gz = "{dataset}/preprocessed_data/R1.fastq.gz",
        R2gz = "{dataset}/preprocessed_data/R2.fastq.gz"
    params:
        lsfoutfile = "{dataset}/preprocessed_data/prinseq.lsfout.log",
        lsferrfile = "{dataset}/preprocessed_data/prinseq.lsferr.log",
        scratch = '2000',
        mem = config.preprocessing['mem'],
        time = config.preprocessing['time'],
        PRINSEQ = config.applications['prinseq'],
    conda:
        config.preprocessing['conda']
    benchmark:
        "{dataset}/preprocessed_data/prinseq.benchmark"
    threads:
        1
    shell:
        """
        if [[ -f {wildcards.dataset}/raw_data/length_cutoff ]]; then
                CUTOFF=$(cat {wildcards.dataset}/raw_data/length_cutoff)
        else
                CUTOFF=200
        fi

        echo "The length cutoff is: ${{CUTOFF}}"

        {params.PRINSEQ} -fastq {input.R1} -fastq2 {input.R2} -out_format 3 -out_good {wildcards.dataset}/preprocessed_data/R -out_bad null -ns_max_n 4 -min_qual_mean 30 -trim_qual_left 30 -trim_qual_right 30 -trim_qual_window 10 -min_len ${{CUTOFF}}

        mv {wildcards.dataset}/preprocessed_data/R{{_,}}1.fastq
        mv {wildcards.dataset}/preprocessed_data/R{{_,}}2.fastq
        rm -f {wildcards.dataset}/preprocessed_data/R_?_singletons.fastq

        gzip {wildcards.dataset}/preprocessed_data/R1.fastq
        gzip {wildcards.dataset}/preprocessed_data/R2.fastq
        """

rule trimmingclean:
    params:
        DIR = config.input['datadir']
    shell:
        """
        rm -rf {params.DIR}/*/*/preprocessed_data
        """


# 3. initial consensus sequence
rule initial_vicuna:
    input:
        global_ref = reference_file,
        R1 = "{dataset}/preprocessed_data/R1.fastq",
        R2 = "{dataset}/preprocessed_data/R2.fastq"
    output:
        "{dataset}/references/vicuna_consensus.fasta"
    params:
        lsfoutfile = "{dataset}/initial_consensus/vicuna.lsfout.log",
        lsferrfile = "{dataset}/initial_consensus/vicuna.lsferr.log",
        scratch = '1000',
        mem = config.initial_vicuna['mem'],
        time = config.initial_vicuna['time'],
        VICUNA = config.applications['vicuna'],
        BWA = config.applications['bwa'],
        SAMTOOLS = config.applications['samtools'],
        WORK_DIR = "{dataset}/initial_consensus",
    conda:
        config.initial_vicuna['conda']
    benchmark:
        '{dataset}/initial_consensus/vicuna_consensus.benchmark'
    threads:
        config.initial_vicuna['threads']
    shell:
        """
        CONSENSUS_NAME={wildcards.dataset}
        CONSENSUS_NAME="${{CONSENSUS_NAME#*/}}"
        CONSENSUS_NAME="${{CONSENSUS_NAME//\//-}}"
        source functions.sh

        # 1. copy initial reference for bwa
        rm -rf {params.WORK_DIR}/
        mkdir -p {params.WORK_DIR}/
        cp {input.global_ref} {params.WORK_DIR}/consensus.fasta
        cd {params.WORK_DIR}

        # 2. create bwa index
        {params.BWA} index consensus.fasta

        # 3. create initial alignment
        {params.BWA} mem -t {threads} consensus.fasta ../preprocessed_data/R{{1,2}}.fastq > first_aln.sam
        rm consensus.fasta.*

        # 4. remove unmapped reads
        {params.SAMTOOLS} view -b -F 4 first_aln.sam > mapped.bam
        rm first_aln.sam

        # 5. extract reads
        mkdir -p cleaned
        SamToFastq I=mapped.bam FASTQ=cleaned/R1.fastq SECOND_END_FASTQ=cleaned/R2.fastq VALIDATION_STRINGENCY=SILENT
        rm mapped.bam

        # 6. create config file
        # NOTE: Tabs are required below
		cat > vicuna_config.txt <<- _EOF_
			minMSize	9
			maxOverhangSize	2
			Divergence	8
			max_read_overhang	2
			max_contig_overhang	10
			pFqDir	cleaned/
			batchSize	100000
			LibSizeLowerBound	100
			LibSizeUpperBound	800
			min_output_contig_len	1000
			outputDIR	./
		_EOF_

        # 7. VICUNA
        OMP_NUM_THREADS={threads} {params.VICUNA} vicuna_config.txt
        rm vicuna_config.txt
        rm -r cleaned/

        # 8. fix broken header
        sed -e 's:>dg-\([[:digit:]]\+\)\s.*:>dg-\1:g' contig.fasta > contig_clean.fasta

        # 9. InDelFixer + ConsensusFixer to polish up consensus
        for i in {{1..3}}
        do
                mv consensus.fasta old_consensus.fasta
                InDelFixer -i contig_clean.fasta -g old_consensus.fasta
                sam2bam reads.sam
                ConsensusFixer -i reads.bam -r old_consensus.fasta -mcc 1 -mic 1 -d -pluralityN 0.01
        done

        sed -i -e "s/>.*/>${{CONSENSUS_NAME}}/" consensus.fasta
        echo "" >> consensus.fasta

        # 10. finally, move into place
        mkdir -p ../references
        mv {{,../references/vicuna_}}consensus.fasta
        """

rule initial_vicuna_msa:
    input:
        vicuna_refs
    output:
        "references/initial_aln_gap_removed.fasta"
    params:
        lsfoutfile = "references/MAFFT_initial_aln.lsfout.log",
        lsferrfile = "references/MAFFT_initial_aln.lsferr.log",
        scratch = '1250',
        mem = config.initial_vicuna_msa['mem'],
        time = config.initial_vicuna_msa['time'],
        MAFFT = config.applications['mafft'],
        REMOVE_GAPS = config.applications['remove_gaps_msa'],
    conda:
        config.initial_vicuna_msa['conda']
    benchmark:
        "references/MAFFT_initial_aln.benchmark"
    threads:
        config.initial_vicuna_msa['threads']
    shell:
        """
        cat {input} > initial_ALL.fasta
        {params.MAFFT} --nuc --preservecase --maxiterate 1000 --localpair --thread {threads} initial_ALL.fasta > references/initial_aln.fasta
        rm initial_ALL.fasta

        {params.REMOVE_GAPS} references/initial_aln.fasta -o {output} -p 0.5
        """

localrules:
    create_vicuna_initial
rule create_vicuna_initial:
    input:
        "references/initial_aln_gap_removed.fasta"
    output:
        "{dataset}/references/initial_consensus.fasta"
    params:
        EXTRACT_SEQ = config.applications['extract_seq'],
    conda:
        config.create_vicuna_initial['conda']
    shell:
        """
        CONSENSUS_NAME={wildcards.dataset}
        CONSENSUS_NAME="${{CONSENSUS_NAME#*/}}"
        CONSENSUS_NAME="${{CONSENSUS_NAME//\//-}}"

        mkdir -p {wildcards.dataset}/references/
        {params.EXTRACT_SEQ} {input} -o {output} -s "${{CONSENSUS_NAME}}"
        """

localrules:
    create_simple_initial
rule create_simple_initial:
    input:
        "references/cohort_consensus.fasta"
    output:
        "{dataset}/references/initial_consensus.fasta"
    shell:
        """
        CONSENSUS_NAME={wildcards.dataset}
        CONSENSUS_NAME="${{CONSENSUS_NAME#*/}}"
        CONSENSUS_NAME="${{CONSENSUS_NAME//\//-}}"

        mkdir -p {wildcards.dataset}/references/
        cp {input} {output}
        sed -i -e "s/>.*/>${{CONSENSUS_NAME}}/" {output}
        """

localrules:
    create_denovo_initial
rule create_denovo_initial:
    input:
        "{dataset}/references/denovo_consensus.fasta"
    output:
        "{dataset}/references/initial_consensus.fasta"
    shell:
        """
        CONSENSUS_NAME={wildcards.dataset}
        CONSENSUS_NAME="${{CONSENSUS_NAME#*/}}"
        CONSENSUS_NAME="${{CONSENSUS_NAME//\//-}}"

        mkdir -p {wildcards.dataset}/references/
        cp {input} {output}
        sed -i -e "s/>.*/>${{CONSENSUS_NAME}}/" {output}
        """

rule vicunaclean:
    params:
        DIR = config.input['datadir']
    shell:
        """
        rm -rf {params.DIR}/*/*/initial_consensus
        rm -rf {params.DIR}/*/*/references/vicuna_consensus.fasta
        rm -rf {params.DIR}/*/*/references/initial_consensus.fasta
        rm -rf references/initial_aln.fasta
        rm -rf references/initial_aln_gap_removed.fasta
        rm -rf references/MAFFT_initial_aln.*
        """

# change this to switch between VICUNA and creating a simple initial
# initial reference
ruleorder:
    create_denovo_initial > create_simple_initial > create_vicuna_initial
# ruleorder: create_vicuna_initial > create_simple_initial


# 4. aligning
rule hmm_align:
    input:
        initial_ref = "{dataset}/references/initial_consensus.fasta",
        R1 = "{dataset}/preprocessed_data/R1.fastq",
        R2 = "{dataset}/preprocessed_data/R2.fastq",
    output:
        good_aln = temp("{dataset}/alignments/full_aln.sam"),
        reject_aln = temp("{dataset}/alignments/rejects.sam"),
        REF_ambig = "{dataset}/references/ref_ambig.fasta",
        REF_majority = "{dataset}/references/ref_majority.fasta",
    params:
        lsfoutfile = "{dataset}/alignments/ngshmmalign.lsfout.log",
        lsferrfile = "{dataset}/alignments/ngshmmalign.lsferr.log",
        scratch = '1250',
        mem = config.hmm_align['mem'],
        time = config.hmm_align['time'],
        LEAVE_TEMP = '-l' if config.hmm_align['leave_msa_temp'] else '',
        MAFFT = config.applications['mafft'],
        NGSHMMALIGN = config.applications['ngshmmalign'],
    conda:
        config.hmm_align['conda']
    benchmark:
        "{dataset}/alignments/ngshmmalign.benchmark"
    threads:
        config.hmm_align['threads']
    shell:
        """
        CONSENSUS_NAME={wildcards.dataset}
        CONSENSUS_NAME="${{CONSENSUS_NAME#*/}}"
        CONSENSUS_NAME="${{CONSENSUS_NAME//\//-}}"

        # 1. clean previous run
        rm -rf   {wildcards.dataset}/alignments
        rm -f    {wildcards.dataset}/references/ref_ambig.fasta
        rm -f    {wildcards.dataset}/references/ref_majority.fasta
        mkdir -p {wildcards.dataset}/alignments
        mkdir -p {wildcards.dataset}/references

        # 2. perform alignment # -l = leave temps
        {params.NGSHMMALIGN} -v -R {input.initial_ref} -o {output.good_aln} -w {output.reject_aln} -t {threads} -N "${{CONSENSUS_NAME}}" {params.LEAVE_TEMP} {input.R1} {input.R2}

        # 3. move references into place
        mv {wildcards.dataset}/{{alignments,references}}/ref_ambig.fasta
        mv {wildcards.dataset}/{{alignments,references}}/ref_majority.fasta
        """

rule sam2bam:
    input:
        "{file}.sam"
    output:
        BAM = "{file}.bam",
        BAI = "{file}.bam.bai"
    params:
        lsfoutfile = "{file}_sam2bam.lsfout.log",
        lsferrfile = "{file}_sam2bam.lsferr.log",
        scratch = '1250',
        mem = config.sam2bam['mem'],
        time = config.sam2bam['time'],
        SAMTOOLS = config.applications['samtools'],
    conda:
        config.sam2bam['conda']
    benchmark:
        "{file}_sam2bam.benchmark"
    threads:
        1
    shell:
        """
        # convert sam -> bam
        source functions.sh
        sam2bam {input}
        """

# 4a. align against 5VM as a QA check
rule bwa_align:
    input:
        patient_ref = "{dataset}/references/ref_{kind}.fasta",
        virusmix_ref = "references/5-Virus-Mix.fasta",
        R1 = "{dataset}/preprocessed_data/R1.fastq",
        R2 = "{dataset}/preprocessed_data/R2.fastq",
    output:
        SAM = temp("{dataset}/QA_alignments/bwa_aln_{kind}.sam"),
        MSA = "{dataset}/QA_alignments/bwa_refs_msa_{kind}.fasta",
    params:
        lsfoutfile = "{dataset}/QA_alignments/bwa_{kind}.lsfout.log",
        lsferrfile = "{dataset}/QA_alignments/bwa_{kind}.lsferr.log",
        scratch = '1250',
        mem = config.bwa_align['mem'],
        time = config.bwa_align['time'],
        BWA = config.applications['bwa'],
        MAFFT = config.applications['mafft'],
    conda:
        config.bwa_align['conda']
    benchmark:
        "{dataset}/QA_alignments/bwa_{kind}.benchmark"
    threads:
        config.bwa_align['threads']
    shell:
        """
        # 1. cleanup old run
        rm -f {output.SAM} {output.MSA}

        # 2. concatenate references
        mkdir -p {wildcards.dataset}/QA_alignments
        cat {input.patient_ref} {input.virusmix_ref} > {wildcards.dataset}/QA_alignments/bwa_refs_{wildcards.kind}.fasta

        # 3. indexing
        {params.BWA} index {wildcards.dataset}/QA_alignments/bwa_refs_{wildcards.kind}.fasta

        # 4. align
        {params.BWA} mem -t {threads} {wildcards.dataset}/QA_alignments/bwa_refs_{wildcards.kind}.fasta {input.R1} {input.R2} > {output.SAM}

        # 5. MSA
        {params.MAFFT} --nuc --preservecase --maxiterate 1000 --localpair --thread {threads} {wildcards.dataset}/QA_alignments/bwa_refs_{wildcards.kind}.fasta > {output.MSA}

        # 6. cleanup BWA indices
        rm -f {wildcards.dataset}/QA_alignments/bwa_refs_{wildcards.kind}.fasta.*
        """

# 4b. Call coverage statistics
rule coverage_QA:
    input:
        BAM = "{dataset}/QA_alignments/bwa_aln_{kind}.bam",
        MSA = "{dataset}/QA_alignments/bwa_refs_msa_{kind}.fasta",
    output:
        "{dataset}/QA_alignments/coverage_{kind}.tsv",
    params:
        lsfoutfile = "{dataset}/QA_alignments/coverage_QA_{kind}.lsfout.log",
        lsferrfile = "{dataset}/QA_alignments/coverage_QA_{kind}.lsferr.log",
        scratch = '1250',
        mem = config.coverage_QA['mem'],
        time = config.coverage_QA['time'],
        COV_STATS = config.applications['coverage_stats'],
    conda:
        config.coverage_QA['conda']
    benchmark:
        "{dataset}/QA_alignments/coverage_QA_{kind}.benchmark"
    threads:
        1
    shell:
        """
        CONSENSUS_NAME={wildcards.dataset}
        CONSENSUS_NAME="${{CONSENSUS_NAME#*/}}"
        CONSENSUS_NAME="${{CONSENSUS_NAME//\//-}}"

        # 1. clean previous run
        rm -f {output}
        mkdir -p {wildcards.dataset}/QA_alignments

        # 2. collect coverage stats
        # we only collect statistics in the loop regions
        # of HIV-1 in order
        {params.COV_STATS} -t HXB2:6614-6812,7109-7217,7376-7478,7601-7634 -i {input.BAM} -o {output} -m {input.MSA} --select "${{CONSENSUS_NAME}}"
        """


# 5. construct MSA from all patient files
def construct_msa_input_files(wildcards):
    output_list = ["{}{}.fasta".format(s, wildcards.kind)
                   for s in references]
    output_list.append(reference_file)

    return output_list


rule msa:
    input:
        construct_msa_input_files
    output:
        "references/ALL_aln_{kind}.fasta"
    params:
        lsfoutfile = "references/MAFFT_{kind}_cohort.lsfout.log",
        lsferrfile = "references/MAFFT_{kind}_cohort.lsferr.log",
        scratch = '1250',
        mem = config.msa['mem'],
        time = config.msa['time'],
        MAFFT = config.applications['mafft'],
    conda:
        config.msa['conda']
    benchmark:
        "references/MAFFT_{kind}_cohort.benchmark"
    threads:
        config.msa['threads']
    shell:
        """
        cat {input} > ALL_{wildcards.kind}.fasta
        {params.MAFFT} --nuc --preservecase --maxiterate 1000 --localpair --thread {threads} ALL_{wildcards.kind}.fasta > {output}
        rm ALL_{wildcards.kind}.fasta
        """

rule msaclean:
    shell:
        """
        rm -rf references/ALL_aln_*.fasta
        rm -rf references/MAFFT_*_cohort.*
        """


# 6. convert alignments to REF alignment
rule convert_to_ref:
    input:
        REF_ambig = "references/ALL_aln_ambig.fasta",
        REF_majority = "references/ALL_aln_majority.fasta",
        BAM = "{dataset}/alignments/full_aln.bam",
        REJECTS_BAM = "{dataset}/alignments/rejects.bam",
    output:
        "{dataset}/alignments/REF_aln.bam"
    params:
        lsfoutfile = "{dataset}/alignments/convert_to_ref.lsfout.log",
        lsferrfile = "{dataset}/alignments/convert_to_ref.lsferr.log",
        scratch = '1250',
        mem = config.convert_to_ref['mem'],
        time = config.convert_to_ref['time'],
        REF_NAME = reference_name,
        CONVERT_REFERENCE = config.applications['convert_reference'],
    conda:
        config.convert_to_ref['conda']
    benchmark:
        "{dataset}/alignments/convert_to_ref.benchmark"
    threads:
        1
    shadow:
        "shallow"
    shell:
        """
        {params.CONVERT_REFERENCE} -t {params.REF_NAME} -m {input.REF_ambig} -i {input.BAM} -o {output}
        """


rule alignclean:
    params:
        DIR = config.input['datadir']
    shell:
        """
        rm -rf {params.DIR}/*/*/alignments
        rm -rf {params.DIR}/*/*/QA_alignments
        rm -rf {params.DIR}/*/*/references/ref_ambig.fasta
        rm -rf {params.DIR}/*/*/references/ref_majority.fasta
        """

# 7. Ouptut minor allele frequencies
rule minor_variants:
    input:
        REF = reference_file,
        BAM = expand("{dataset}/alignments/REF_aln.bam", dataset=datasets),
    output:
        VARIANTS = "variants/minority_variants.tsv",
        CONSENSUS = "variants/cohort_consensus.fasta",
        COVERAGE = "variants/coverage.tsv"
    params:
        lsfoutfile = "variants/minority_variants.lsfout.log",
        lsferrfile = "/dev/null",
        scratch = '1250',
        mem = config.minor_variants['mem'],
        time = config.minor_variants['time'],
        OUTDIR = "variants",
        NAMES = IDs,
        MINORITY_CALLER = config.applications['minority_freq'],
    conda:
        config.minor_variants['conda']
    benchmark:
        'variants/minority_variants.benchmark'
    threads:
        config.minor_variants['threads']
    shell:
        """
        {params.MINORITY_CALLER} -r {input.REF} -N {params.NAMES} -t {threads} -o {params.OUTDIR} -d {input.BAM}
        """

# 8. Call single nucleotide variants
rule coverage_intervals:
    input:
        REF = reference_file,
        TSV = "variants/coverage.tsv"
    output:
        "variants/coverage_intervals.tsv"
    params:
        lsfoutfile = "variants/coverage_intervals.lsfout.log",
        lsferrfile = "variants/coverage_intervals.lsfout.log",
        scratch = '1250',
        mem = config.coverage_intervals['mem'],
        time = config.coverage_intervals['time'],
        COVERAGE = config.coverage_intervals['coverage'],
        SHIFT = config.snv['shift'],
        NAMES = IDs,
        LIBERAL = '-e' if config.coverage_intervals['liberal'] else '',
        EXTRACT_COVERAGE_INTERVALS = config.applications['extract_coverage_intervals']
    benchmark:
        'variants/coverage_intervals.benchmark'
    threads:
        1
    run:
        window_len = ''
        for d in datasets:
            read_len = os.path.join(d, "raw_data", "read_length")
            if os.path.exists(read_len):
                with open(read_len, 'r') as infile:
                    read_len = int(infile.readline().rstrip())
                    window_shifts = int(
                        (read_len * 4 / 5 + params.SHIFT) / params.SHIFT)
                    aux = window_shifts * params.SHIFT
            else:
                aux = 201
            aux = str(aux)
            if len(window_len) == 0:
                window_len = aux
            else:
                window_len = window_len + ',' + aux

        shell("{params.EXTRACT_COVERAGE_INTERVALS} -i {input.TSV} -r {input.REF} -c {params.COVERAGE} -w " +
              str(window_len) + " -N {params.NAMES} {params.LIBERAL} -o {output}")

localrules:
    shorah_regions
rule shorah_regions:
    input:
        "variants/coverage_intervals.tsv"
    output:
        temp(
            expand("{dataset}/variants/coverage_intervals.tsv", dataset=datasets))
    params:
        lsfoutfile = "/dev/null",
        lsferrfile = "/dev/null",
        scratch = '1250',
    threads:
        1
    run:
        with open(input[0], 'r') as infile:
            for line in infile:
                parts = line.rstrip().split('\t')
                patientID = parts[0].split('-')
                sample_date = patientID[-1]
                patientID = '-'.join(patientID[:-1])
                if len(parts) == 2:
                    regions = parts[1].split(',')
                else:
                    regions = []

                with open(os.path.join(config.input['datadir'], patientID, sample_date, "variants", "coverage_intervals.tsv"), 'w') as outfile:
                    outfile.write('\n'.join(regions))

rule snv:
    input:
        REF = "variants/cohort_consensus.fasta",
        BAM = "{dataset}/alignments/REF_aln.bam",
        TSV = "{dataset}/variants/coverage_intervals.tsv",
    output:
        dynamic("{dataset}/variants/SNVs/{region}/snv/SNVs_0.010000_final.csv")
    params:
        lsfoutfile = "{dataset}/variants/SNVs/shorah.lsfout.log",
        lsferrfile = "{dataset}/variants/SNVs/shorah.lsferr.log",
        scratch = '1250',
        mem = config.snv['mem'],
        time = config.snv['time'],
        SHIFT = config.snv['shift'],
        KEEP_FILES = config.snv['keep_files'],
        SHORAH = config.applications['shorah']
    benchmark:
        "{dataset}/variants/SNVs/shorah.benchmark"
    threads:
        config.snv['threads']
    run:
        read_len = os.path.join(wildcards.dataset, "raw_data", "read_length")
        if os.path.exists(read_len):
            with open(read_len, 'r') as infile:
                read_len = infile.readline().rstrip()
                read_len = int(read_len)
        else:
            read_len = 250

        window_shifts = int((read_len * 4 / 5 + params.SHIFT) / params.SHIFT)
        window_len = window_shifts * params.SHIFT

        # run shorah in each of the predetermined regions
        cwd = os.getcwd()
        linecounter = 0
        if os.path.isfile(input.TSV) and os.path.getsize(input.TSV) > 0:
            with open(input.TSV, 'r') as infile:
                for line in infile:
                    region = line.rstrip()

                    linecounter += 1
                    # Create directory for running shorah in a corresponding window
                    dir = os.path.join(cwd, wildcards.dataset, "variants", "SNVs", ''.join(
                        ("TMP", str(linecounter))))
                    if not os.path.exists(dir):
                        os.makedirs(dir)
                    else:
                        if not params.KEEP_FILES:
                            dir_dst = os.path.join(
                                cwd, wildcards.dataset, "variants", "SNVs", ''.join(("old", str(linecounter))))
                            os.rename(dir, dir_dst)
                            os.makedirs(dir)
                    os.chdir(dir)

                    # Get absolute paths for input files
                    ref = os.path.join(cwd, input.REF)
                    bam = os.path.join(cwd, input.BAM)

                    args = {
                        'BAM': bam,
                        'REF': ref,
                        'WINDOW_LEN': str(window_len),
                        'REGION': region,
                        'SHORAH': params.SHORAH
                    }
                    shell("""
                    {SHORAH} -w {WINDOW_LEN} -x 100000 -r {REGION} -S 42 -b {BAM} -f {REF}
                    """.format(**args))
                    os.chdir(cwd)
        else:
            outdir = os.path.join(cwd, wildcards.dataset,
                                  "variants", "SNVs", "TMP1", "snv")
            if not os.path.exists(outdir):
                os.makedirs(outdir)
            outfile = os.path.join(outdir, "SNVs_0.010000_final.csv")
            with open(outfile, 'a'):
                os.utime(outfile, None)

rule aggregate:
    input:
        dynamic("{dataset}/variants/SNVs/{region}/snv/SNVs_0.010000_final.csv")
    output:
        "{dataset}/variants/SNVs/snvs.csv"
    params:
        lsfoutfile = "{dataset}/variants/aggregate.lsfout.log",
        lsferrfile = "{dataset}/variants/aggregate.lsfout.log",
        scratch = '1250',
        mem = config.aggregate['mem'],
        time = config.aggregate['time'],
    threads:
        1
    shell:
        """
        array=( {input} )
        num_files=( ${{#array[@]}} )
        cat {input} | sort -t, -nk2 | tail -n +${{num_files}} > {output}
        """

rule snvclean:
    params:
        DIR = config.input['datadir']
    shell:
        """
        rm -rf {params.DIR}/*/*/variants/SNVs
        """

rule haploclique:
    input:
        "{dataset}/alignments/REF_aln.bam"
    output:
        FASTA = "{dataset}/variants/global/quasispecies.fasta",
        BAM = "{dataset}/variants/global/quasispecies.bam",
    params:
        lsfoutfile = "{dataset}/variants/global/haploclique.lsfout.log",
        lsferrfile = "{dataset}/variants/global/haploclique.lsferr.log",
        scratch = '1250',
        mem = config.haploclique['mem'],
        time = config.haploclique['time'],
        RELAX = '--edge_quasi_cutoff_cliques=0.85 --edge_quasi_cutoff_mixed=0.85 --edge_quasi_cutoff_single=0.8 --min_overlap_cliques=0.6 --min_overlap_single=0.5' if config.haploclique[
            'relax'] else '',
        NO_SINGLETONS = '--no_singletons' if config.haploclique['no_singletons'] else '',
        NO_PROB0 = '--no_prob0' if config.haploclique['no_prob0'] else '',
        CLIQUE_SIZE_LIMIT = config.haploclique['clique_size_limit'],
        MAX_NUM_CLIQUES = config.haploclique['max_num_cliques'],
        OUTPREFIX = "{dataset}/variants/global/quasispecies",
        HAPLOCLIQUE = config.applications['haploclique'],
    benchmark:
        "{dataset}/variants/global/haploclique.benchmark"
    threads:
        1
    shell:
        """
        {params.HAPLOCLIQUE} {params.RELAX} {params.NO_SINGLETONS} {params.NO_PROB0} --limit_clique_size={params.CLIQUE_SIZE_LIMIT} --max_cliques={params.MAX_NUM_CLIQUES} --bam {input} {params.OUTPREFIX}
        """

rule haploclique_visualization:
    input:
        BAM = "{dataset}/variants/global/quasispecies.bam",
        FASTA = "{dataset}/variants/global/quasispecies.fasta",
    output:
        PDF = "{dataset}/variants/global/quasispecies_plot.pdf",
    params:
        lsfoutfile = "{dataset}/variants/global/haploclique_visualization.lsfout.log",
        lsferrfile = "{dataset}/variants/global/haploclique_visualization.lsferr.log",
        scratch = '1250',
        mem = config.haploclique_visualization['mem'],
        time = config.haploclique_visualization['time'],
        REGION_START = config.haploclique_visualization['region_start'],
        REGION_END = config.haploclique_visualization['region_end'],
        USE_MSA = '-r' if len(
            config.haploclique_visualization['msa']) > 0 else '',
        MSA = config.haploclique_visualization['msa'],
        TSV = "{dataset}/variants/global/quasispecies_mapping.tsv",
        INPREFIX = "{dataset}/variants/global/quasispecies",
        COMPUTE_MDS = config.applications['compute_mds'],
    conda:
        config.haploclique_visualization['conda']
    benchmark:
        "{dataset}/variants/global/haploclique_visualization.benchmark"
    threads:
        1
    shell:
        """
        {params.COMPUTE_MDS} -q {params.INPREFIX} -s {params.REGION_START} -e {params.REGION_END} {params.USE_MSA} {params.MSA} -p {output.PDF} -o {params.TSV}
        """

rule haplocliqueclean:
    params:
        DIR = config.input['datadir']
    shell:
        """
        rm {params.DIR}/*/*/variants/global/quasispecies.*
        """
