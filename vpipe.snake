import csv
import collections
import configparser
import os
import sys
import typing

# 1. parse config file
#
# the config object sets defaults for a number of parameters and
# validates parameters given by the user to be sensible

VPIPE_DEBUG = True if os.environ.get('VPIPE_DEBUG') is not None else False


class VpipeConfig(object):
    'Class used to encapsulate the configuration properties used by V-pipe'

    __RECORD__ = typing.NamedTuple(
        "__RECORD__", [('value', typing.Any), ('type', type)])
    __MEMBER_DEFAULT__ = collections.OrderedDict([
        ('general', {
            'threads': __RECORD__(value=4, type=int),
        }),
        ('input', {
            'datadir': __RECORD__(value='samples', type=str),
            'samples_file': __RECORD__(value='samples.tsv', type=str),
            'fastq_suffix': __RECORD__(value='', type=str),
            'trim_cutoff': __RECORD__(value=200, type=float),
        }),
        ('output', {
            'QA': __RECORD__(value=False, type=bool),
            'snv': __RECORD__(value=True, type=bool),
            'local': __RECORD__(value=True, type=bool),
            'global': __RECORD__(value=True, type=bool),
        }),
        ('applications', {
            'gunzip': __RECORD__(value="gunzip", type=str),
            'prinseq': __RECORD__(value="prinseq-lite.pl", type=str),
            'vicuna': __RECORD__(value="vicuna", type=str),
            'bwa': __RECORD__(value="bwa", type=str),
            'samtools': __RECORD__(value="samtools", type=str),
            'mafft': __RECORD__(value="mafft", type=str),
            'ngshmmalign': __RECORD__(value="ngshmmalign", type=str),
            'convert_reference': __RECORD__(value="convert_reference", type=str),
            'extract_seq': __RECORD__(value="extract_seq", type=str),
            'coverage_stats': __RECORD__(value="coverage_stats", type=str),
            'remove_gaps_msa': __RECORD__(value="remove_gaps_msa", type=str),
        }),

        ('gunzip', {
            'mem': __RECORD__(value=30000, type=int),
            'time': __RECORD__(value=60, type=int),
        }),
        ('extract', {
            'mem': __RECORD__(value=10000, type=int),
            'time': __RECORD__(value=20, type=int),
        }),
        ('preprocessing', {
            'mem': __RECORD__(value=2000, type=int),
            'time': __RECORD__(value=235, type=int),
            'conda': __RECORD__(value='', type=str),

            'qual_threshold': __RECORD__(value=30, type=int),
            'min_len': __RECORD__(value=0.8, type=float),
        }),
        ('initial_vicuna', {
            'mem': __RECORD__(value=1000, type=int),
            'time': __RECORD__(value=600, type=int),
            'threads': __RECORD__(value=0, type=int),
            'conda': __RECORD__(value='', type=str),
        }),
        ('initial_vicuna_msa', {
            'mem': __RECORD__(value=10000, type=int),
            'time': __RECORD__(value=235, type=int),
            'threads': __RECORD__(value=0, type=int),
            'conda': __RECORD__(value='', type=str),
        }),
        ('create_vicuna_initial', {
            'conda': __RECORD__(value='', type=str),
        }),
        ('hmm_align', {
            'mem': __RECORD__(value=1250, type=int),
            'time': __RECORD__(value=1435, type=int),
            'threads': __RECORD__(value=0, type=int),
            'conda': __RECORD__(value='', type=str),

            'leave_msa_temp': __RECORD__(value=False, type=bool),
        }),
        ('sam2bam', {
            'mem': __RECORD__(value=5000, type=int),
            'time': __RECORD__(value=30, type=int),
            'conda': __RECORD__(value='', type=str),
        }),
        ('bwa_align', {
            'mem': __RECORD__(value=1250, type=int),
            'time': __RECORD__(value=235, type=int),
            'threads': __RECORD__(value=0, type=int),
            'conda': __RECORD__(value='', type=str),
        }),
        ('coverage_QA', {
            'mem': __RECORD__(value=1250, type=int),
            'time': __RECORD__(value=235, type=int),
            'conda': __RECORD__(value='', type=str),
        }),
        ('msa', {
            'mem': __RECORD__(value=10000, type=int),
            'time': __RECORD__(value=235, type=int),
            'threads': __RECORD__(value=0, type=int),
            'conda': __RECORD__(value='', type=str),
        }),
        ('convert_to_hxb2', {
            'mem': __RECORD__(value=8000, type=int),
            'time': __RECORD__(value=235, type=int),
            'conda': __RECORD__(value='', type=str),
        }),
    ])

    def __init__(self):
        self.__members = {}

        vpipe_configfile = configparser.ConfigParser()
        vpipe_configfile.read('vpipe.config')

        for (section, properties) in VpipeConfig.__MEMBER_DEFAULT__.items():
            self.__members[section] = {}

            for (value, defaults) in properties.items():
                try:
                    if defaults.type == int:
                        cur_value = vpipe_configfile.getint(section, value)
                    elif defaults.type == float:
                        cur_value = vpipe_configfile.getfloat(section, value)
                    elif defaults.type == bool:
                        cur_value = vpipe_configfile.getboolean(section, value)
                    else:
                        cur_value = vpipe_configfile.get(section, value)
                    state = 'user'
                except (configparser.NoSectionError, configparser.NoOptionError):
                    if value == 'threads' and section != 'general':
                        cur_value = defaults.value if defaults.value else VpipeConfig.__MEMBER_DEFAULT__[
                                'general']['threads'][0]
                    elif value == 'conda':
                        cur_value = "envs/{}.yaml".format(section)
                    else:
                        cur_value = defaults.value
                    state = 'DEFAULT'
                except ValueError:
                    print("ERROR: Property '{}' of section '{}' has to be of type '{}', whereas you gave '{}'!".format(
                        value, section, defaults.type.__name__, vpipe_configfile[section][value]))
                    raise

                if VPIPE_DEBUG:
                    print("Using {} value '{}' for property '{}' in section '{}'".format(
                        state, cur_value, value, section))

                self.__members[section][value] = cur_value

    def __getattr__(self, name):
        try:
            return self.__members[name]
        except:
            print("ERROR: Section '{}' is not a valid section!".format(name))
            raise

config = VpipeConfig()


# 2. glob patients/samples + store as TSV if file is not provided

# if file containing samples exitst, proceed
# ro build list of target files
if os.path.isfile(config.input['samples_file']):
    # sample file does not exist, have to first glob
    # all patients' data and then construct sample
    # list that would pass QA checks

    patient_sample_pairs = glob_wildcards(
        "{}/{{patient_date}}/raw_data/{{file}}_R1{}.fastq.gz".format(config.input['datadir'], config.input['fastq_suffix']))

    with open('samples.tsv', 'w') as outfile:
        for i in patient_sample_pairs.patient_date:
            (patient, date) = [x.strip() for x in i.split("/") if x.strip()]
            outfile.write('{}\t{}\n'.format(patient, date))

    # TODO: have to preprocess patient files to filter likely failures
    # 1.) Determine 5%/95% length of FASTQ files
    # 2.) Determine whether FASTQ would survive


# 3. load patients from TSV and create list of samples
#
# This list is reused on subsequent runs

patient_list = []
patient_dict = {}
patient_record = typing.NamedTuple(
    "patient_record", [('patient_id', str), ('date', str)])

with open(config.input['samples_file'], newline='') as csvfile:
    spamreader = csv.reader(csvfile, delimiter='\t')
    
    for row in spamreader:
        assert len(row) >= 2, "ERROR: Line '{}' does not contain at least two entries!".format(
                              spamreader.line_num)
        patient_tuple = patient_record(patient_id=row[0], date=row[1])
        patient_list.append(patient_tuple)
        
        if len(row) == 2:
            # All samples are assumed to have same read length and 'trim_cutoff'
            # must correspond to the threshold value in the length of the reads
            # after quality trimming
            assert config.input['trim_cutoff'] > 1, "ERROR: 'trim_cutoff' is expected to be larger than 1, whereas 'trim_cutoff'={}".format(config.input['trim_cutoff'])
            
            assert patient_tuple not in patient_dict, "ERROR: sample '{}-{}' is not unique".format(
                                                      row[0], row[1])
            patient_dict[patient_tuple] = int(config.input['trim_cutoff'])

        elif len(row) == 3: 
            assert config.input['trim_cutoff'] < 0 or config.input['trim_cutoff'] > 1, "ERROR: 'trim_cutoff' is expected to be a fraction (between 0 and 1), whereas 'trim_cutoff'={}".format(config.input['trim_cutoff'])
            # Extract read length from input.samples_file. Samples may have 
            # different read lengths. Reads will be filtered out if read length
            # after trimming is less than trim_cutoff * read_length. If snvs 
            # are expected as output, third column should be provided
            assert patient_list not in patient_dict, "ERROR: sample '{}-{}' is not unique".format(
                                                      row[0], row[1])
            patient_dict[patient_tuple] = int(config.input['trim_cutoff'] * row[3])

# 4. generate list of target files
all_files = []
alignments = []
vicuna_refs = []
references = []
trimmed_files = []
results = []
for p in patient_list:
    alignments.append(
        "{sample_dir}/{patient}/{date}/alignments/HXB2_aln.bam".format(sample_dir=config.input['datadir'], patient=p.patient_id, date=p.date))
    if config.output['QA']:
        alignments.append(
            "{sample_dir}/{patient}/{date}/QA_alignments/coverage_ambig.tsv".format(sample_dir=config.input['datadir'], patient=p.patient_id, date=p.date))
        alignments.append(
            "{sample_dir}/{patient}/{date}/QA_alignments/coverage_majority.tsv".format(sample_dir=config.input['datadir'], patient=p.patient_id, date=p.date))

    vicuna_refs.append(
        "{sample_dir}/{patient}/{date}/references/vicuna_consensus.fasta".format(sample_dir=config.input['datadir'], patient=p.patient_id, date=p.date))
    references.append(
        "{sample_dir}/{patient}/{date}/references/ref_".format(sample_dir=config.input['datadir'], patient=p.patient_id, date=p.date))

    trimmed_files.append(
        "{sample_dir}/{patient}/{date}/preprocessed_data/R1.fastq.gz".format(sample_dir=config.input['datadir'], patient=p.patient_id, date=p.date))
    trimmed_files.append(
        "{sample_dir}/{patient}/{date}/preprocessed_data/R2.fastq.gz".format(sample_dir=config.input['datadir'], patient=p.patient_id, date=p.date))

    # SNV
    if config.output['snv']:
        results.append(
            "{sample_dir}/{patient}/{date}/results/SNV/snvs.tsv".format(sample_dir=config.input['datadir'], patient=p.patient_id, date=p.date))
    # local haplotypes
    if config.output['local']:
        results.append(
            "{sample_dir}/{patient}/{date}/results/local/snvs.tsv".format(sample_dir=config.input['datadir'], patient=p.patient_id, date=p.date))
    # global haplotypes
    if config.output['global']:
        results.append(
            "{sample_dir}/{patient}/{date}/results/global/snvs.tsv".format(sample_dir=config.input['datadir'], patient=p.patient_id, date=p.date))

    # merge lists contaiing expected output
    all_files = alignments + results


# DUMMY RULES
rule all:
    input:
        all_files

rule alltrimmed:
    input:
        trimmed_files


# 1. extract
rule gunzip:
    input:
        "{file}.fastq.gz"
    output:
        temp("{file}.fastq")
    params:
        lsfoutfile = "/dev/null",
        lsferrfile = "/dev/null",
        scratch = '10000',
        mem = config.gunzip['mem'],
        time = config.gunzip['time'],
        GUNZIP = config.applications['gunzip'],
    threads:
        1
    shell:
        """
        {params.GUNZIP} -c {input} > {output}
        """


def construct_input_fastq(wildcards):
    inferred_values = glob_wildcards(
        wildcards.dataset + "/raw_data/{file}R" + wildcards.pair + config.input['fastq_suffix'] + ".fastq.gz")
    list_output = []

    for i in inferred_values.file:
        list_output.append(wildcards.dataset + "/raw_data/" +
                           i + "R" + wildcards.pair + config.input['fastq_suffix'] + ".fastq")

    return list_output


rule extract:
    input:
        construct_input_fastq
    output:
        temp("{dataset}/extracted_data/R{pair}.fastq")
    params:
        lsfoutfile = "{dataset}/extracted_data/extract.lsfout.log",
        lsferrfile = "{dataset}/extracted_data/extract.lsferr.log",
        scratch = '2000',
        mem = config.extract['mem'],
        time = config.extract['time'],
    benchmark:
        "{dataset}/extracted_data/extract.benchmark"
    threads:
        1
    shell:
        """
        cat {input} | paste - - - - | sort -k1,1 -t " " | tr "\t" "\n" > {output}
        """

rule extractclean:
    shell:
        """
        rm -rf patients/*/*/extracted_data
        """


# 2. clipping
rule preprocessing:
    input:
        R1 = "{dataset}/extracted_data/R1.fastq",
        R2 = "{dataset}/extracted_data/R2.fastq"
    output:
        R1gz = "{dataset}/preprocessed_data/R1.fastq.gz",
        R2gz = "{dataset}/preprocessed_data/R2.fastq.gz"
    params:
        lsfoutfile = "{dataset}/preprocessed_data/prinseq.lsfout.log",
        lsferrfile = "{dataset}/preprocessed_data/prinseq.lsferr.log",
        scratch = '2000',
        mem = config.preprocessing['mem'],
        time = config.preprocessing['time'],
        PRINSEQ = config.applications['prinseq'],
    conda:
        config.preprocessing['conda']
    benchmark:
        "{dataset}/preprocessed_data/prinseq.benchmark"
    threads:
        1
    shell:
        """
        if [[ -f {wildcards.dataset}/raw_data/length_cutoff ]]; then
                CUTOFF=$(cat {wildcards.dataset}/raw_data/length_cutoff)
        else
                CUTOFF=200
        fi

        echo "The length cutoff is: ${{CUTOFF}}"

        {params.PRINSEQ} -fastq {input.R1} -fastq2 {input.R2} -out_format 3 -out_good {wildcards.dataset}/preprocessed_data/R -out_bad null -ns_max_n 4 -min_qual_mean 30 -trim_qual_left 30 -trim_qual_right 30 -trim_qual_window 10 -min_len ${{CUTOFF}}

        mv {wildcards.dataset}/preprocessed_data/R{{_,}}1.fastq
        mv {wildcards.dataset}/preprocessed_data/R{{_,}}2.fastq
        rm -f {wildcards.dataset}/preprocessed_data/R_?_singletons.fastq

        gzip {wildcards.dataset}/preprocessed_data/R1.fastq
        gzip {wildcards.dataset}/preprocessed_data/R2.fastq
        """

rule trimmingclean:
    shell:
        """
        rm -rf patients/*/*/preprocessed_data
        """


# 3. initial consensus sequence
rule initial_vicuna:
    input:
        global_ref = "references/HXB2.fasta",
        R1 = "{dataset}/preprocessed_data/R1.fastq",
        R2 = "{dataset}/preprocessed_data/R2.fastq"
    output:
        "{dataset}/references/vicuna_consensus.fasta"
    params:
        lsfoutfile = "{dataset}/initial_consensus/vicuna.lsfout.log",
        lsferrfile = "{dataset}/initial_consensus/vicuna.lsferr.log",
        scratch = '1000',
        mem = config.initial_vicuna['mem'],
        time = config.initial_vicuna['time'],
        VICUNA = config.applications['vicuna'],
        BWA = config.applications['bwa'],
        SAMTOOLS = config.applications['samtools'],
        WORK_DIR = "{dataset}/initial_consensus",
    conda:
        config.initial_vicuna['conda']
    benchmark:
        '{dataset}/initial_consensus/vicuna_consensus.benchmark'
    threads:
        config.initial_vicuna['threads']
    shell:
        """
        CONSENSUS_NAME={wildcards.dataset}
        CONSENSUS_NAME="${{CONSENSUS_NAME#*/}}"
        CONSENSUS_NAME="${{CONSENSUS_NAME//\//-}}"
        source functions.sh

        # 1. copy initial reference for bwa
        rm -rf {params.WORK_DIR}/
        mkdir -p {params.WORK_DIR}/
        cp {input.global_ref} {params.WORK_DIR}/consensus.fasta
        cd {params.WORK_DIR}

        # 2. create bwa index
        {params.BWA} index consensus.fasta

        # 3. create initial alignment
        {params.BWA} mem -t {threads} consensus.fasta ../preprocessed_data/R{{1,2}}.fastq > first_aln.sam
        rm consensus.fasta.*

        # 4. remove unmapped reads
        {params.SAMTOOLS} view -b -F 4 first_aln.sam > mapped.bam
        rm first_aln.sam

        # 5. extract reads
        mkdir -p cleaned
        SamToFastq I=mapped.bam FASTQ=cleaned/R1.fastq SECOND_END_FASTQ=cleaned/R2.fastq VALIDATION_STRINGENCY=SILENT
        rm mapped.bam

        # 6. create config file
        cat > vicuna_config.txt <<- _EOF_
                minMSize    9
                maxOverhangSize    2
                Divergence    8
                max_read_overhang    2
                max_contig_overhang    10
                pFqDir    cleaned/
                batchSize    100000
                LibSizeLowerBound    100
                LibSizeUpperBound    800
                min_output_contig_len    1000
                outputDIR    ./
        _EOF_

        # 7. VICUNA
        OMP_NUM_THREADS={threads} {params.VICUNA} vicuna_config.txt
        rm vicuna_config.txt
        rm -r cleaned/

        # 8. fix broken header
        sed -e 's:>dg-\([[:digit:]]\+\)\s.*:>dg-\1:g' contig.fasta > contig_clean.fasta

        # 9. InDelFixer + ConsensusFixer to polish up consensus
        for i in {{1..3}}
        do
                mv consensus.fasta old_consensus.fasta
                InDelFixer -i contig_clean.fasta -g old_consensus.fasta
                sam2bam reads.sam
                ConsensusFixer -i reads.bam -r old_consensus.fasta -mcc 1 -mic 1 -d -pluralityN 0.01
        done

        sed -i -e "s/>.*/>${{CONSENSUS_NAME}}/" consensus.fasta
        echo "" >> consensus.fasta

        # 10. finally, move into place
        mkdir -p ../references
        mv {{,../references/vicuna_}}consensus.fasta
        """

rule initial_vicuna_msa:
    input:
        vicuna_refs
    output:
        "references/initial_aln_gap_removed.fasta"
    params:
        lsfoutfile = "references/MAFFT_initial_aln.lsfout.log",
        lsferrfile = "references/MAFFT_initial_aln.lsferr.log",
        scratch = '1250',
        mem = config.initial_vicuna_msa['mem'],
        time = config.initial_vicuna_msa['time'],
        MAFFT = config.applications['mafft'],
        REMOVE_GAPS = config.applications['remove_gaps_msa'],
    conda:
        config.initial_vicuna_msa['conda']
    benchmark:
        "references/MAFFT_initial_aln.benchmark"
    threads:
        config.initial_vicuna_msa['threads']
    shell:
        """
        cat {input} > initial_ALL.fasta
        {params.MAFFT} --nuc --preservecase --maxiterate 1000 --localpair --thread {threads} initial_ALL.fasta > references/initial_aln.fasta
        rm initial_ALL.fasta

        {params.REMOVE_GAPS} references/initial_aln.fasta -o {output} -p 0.5
        """

localrules:
    create_vicuna_initial
rule create_vicuna_initial:
    input:
        "references/initial_aln_gap_removed.fasta"
    output:
        "{dataset}/references/initial_consensus.fasta"
    params:
        EXTRACT_SEQ = config.applications['extract_seq'],
    conda:
        config.create_vicuna_initial['conda']
    shell:
        """
        CONSENSUS_NAME={wildcards.dataset}
        CONSENSUS_NAME="${{CONSENSUS_NAME#*/}}"
        CONSENSUS_NAME="${{CONSENSUS_NAME//\//-}}"

        mkdir -p {wildcards.dataset}/references/
        {params.EXTRACT_SEQ} {input} -o {output} -s "${{CONSENSUS_NAME}}"
        """

localrules:
    create_simple_initial
rule create_simple_initial:
    input:
        "references/cohort_consensus.fasta"
    output:
        "{dataset}/references/initial_consensus.fasta"
    shell:
        """
        CONSENSUS_NAME={wildcards.dataset}
        CONSENSUS_NAME="${{CONSENSUS_NAME#*/}}"
        CONSENSUS_NAME="${{CONSENSUS_NAME//\//-}}"

        mkdir -p {wildcards.dataset}/references/
        cp {input} {output}
        sed -i -e "s/>.*/>${{CONSENSUS_NAME}}/" {output}
        """

localrules:
    create_denovo_initial
rule create_denovo_initial:
    input:
        "{dataset}/references/denovo_consensus.fasta"
    output:
        "{dataset}/references/initial_consensus.fasta"
    shell:
        """
        CONSENSUS_NAME={wildcards.dataset}
        CONSENSUS_NAME="${{CONSENSUS_NAME#*/}}"
        CONSENSUS_NAME="${{CONSENSUS_NAME//\//-}}"

        mkdir -p {wildcards.dataset}/references/
        cp {input} {output}
        sed -i -e "s/>.*/>${{CONSENSUS_NAME}}/" {output}
        """

rule vicunaclean:
    shell:
        """
        rm -rf patients/*/*/initial_consensus
        rm -rf patients/*/*/references/vicuna_consensus.fasta
        rm -rf patients/*/*/references/initial_consensus.fasta
        rm -rf references/initial_aln.fasta
        rm -rf references/initial_aln_gap_removed.fasta
        rm -rf references/MAFFT_initial_aln.*
        """

# change this to switch between VICUNA and creating a simple (HXB2)
# initial reference
ruleorder:
    create_denovo_initial > create_simple_initial > create_vicuna_initial
# ruleorder: create_vicuna_initial > create_simple_initial


# 4. aligning
rule hmm_align:
    input:
        initial_ref = "{dataset}/references/initial_consensus.fasta",
        R1 = "{dataset}/preprocessed_data/R1.fastq",
        R2 = "{dataset}/preprocessed_data/R2.fastq",
    output:
        good_aln = temp("{dataset}/alignments/full_aln.sam"),
        reject_aln = temp("{dataset}/alignments/rejects.sam"),
        REF_ambig = "{dataset}/references/ref_ambig.fasta",
        REF_majority = "{dataset}/references/ref_majority.fasta",
    params:
        lsfoutfile = "{dataset}/alignments/ngshmmalign.lsfout.log",
        lsferrfile = "{dataset}/alignments/ngshmmalign.lsferr.log",
        scratch = '1250',
        mem = config.hmm_align['mem'],
        time = config.hmm_align['time'],
        LEAVE_TEMP = '-l' if config.hmm_align['leave_msa_temp'] else '',
        MAFFT = config.applications['mafft'],
        NGSHMMALIGN = config.applications['ngshmmalign'],
    conda:
        config.hmm_align['conda']
    benchmark:
        "{dataset}/alignments/ngshmmalign.benchmark"
    threads:
        config.hmm_align['threads']
    shell:
        """
        CONSENSUS_NAME={wildcards.dataset}
        CONSENSUS_NAME="${{CONSENSUS_NAME#*/}}"
        CONSENSUS_NAME="${{CONSENSUS_NAME//\//-}}"

        # 1. clean previous run
        rm -rf   {wildcards.dataset}/alignments
        rm -f    {wildcards.dataset}/references/ref_ambig.fasta
        rm -f    {wildcards.dataset}/references/ref_majority.fasta
        mkdir -p {wildcards.dataset}/alignments
        mkdir -p {wildcards.dataset}/references

        # 2. perform alignment # -l = leave temps
        {params.NGSHMMALIGN} -v -R {input.initial_ref} -o {output.good_aln} -w {output.reject_aln} -t {threads} -N "${{CONSENSUS_NAME}}" {params.LEAVE_TEMP} {input.R1} {input.R2}

        # 3. move references into place
        mv {wildcards.dataset}/{{alignments,references}}/ref_ambig.fasta
        mv {wildcards.dataset}/{{alignments,references}}/ref_majority.fasta
        """

rule sam2bam:
    input:
        "{file}.sam"
    output:
        BAM = "{file}.bam",
        BAI = "{file}.bam.bai"
    params:
        lsfoutfile = "{file}_sam2bam.lsfout.log",
        lsferrfile = "{file}_sam2bam.lsferr.log",
        scratch = '1250',
        mem = config.sam2bam['mem'],
        time = config.sam2bam['time'],
        SAMTOOLS = config.applications['samtools'],
    conda:
        config.sam2bam['conda']
    benchmark:
        "{file}_sam2bam.benchmark"
    threads:
        1
    shell:
        """
        # convert sam -> bam
        source functions.sh
        sam2bam {input}
        """

# 4a. align against 5VM as a QA check
rule bwa_align:
    input:
        patient_ref = "{dataset}/references/ref_{kind}.fasta",
        virusmix_ref = "references/5-Virus-Mix.fasta",
        R1 = "{dataset}/preprocessed_data/R1.fastq",
        R2 = "{dataset}/preprocessed_data/R2.fastq",
    output:
        SAM = temp("{dataset}/QA_alignments/bwa_aln_{kind}.sam"),
        MSA = "{dataset}/QA_alignments/bwa_refs_msa_{kind}.fasta",
    params:
        lsfoutfile = "{dataset}/QA_alignments/bwa_{kind}.lsfout.log",
        lsferrfile = "{dataset}/QA_alignments/bwa_{kind}.lsferr.log",
        scratch = '1250',
        mem = config.bwa_align['mem'],
        time = config.bwa_align['time'],
        BWA = config.applications['bwa'],
        MAFFT = config.applications['mafft'],
    conda:
        config.bwa_align['conda']
    benchmark:
        "{dataset}/QA_alignments/bwa_{kind}.benchmark"
    threads:
        config.bwa_align['threads']
    shell:
        """
        # 1. cleanup old run
        rm -f {output.SAM} {output.MSA}

        # 2. concatenate references
        mkdir -p {wildcards.dataset}/QA_alignments
        cat {input.patient_ref} {input.virusmix_ref} > {wildcards.dataset}/QA_alignments/bwa_refs_{wildcards.kind}.fasta

        # 3. indexing
        {params.BWA} index {wildcards.dataset}/QA_alignments/bwa_refs_{wildcards.kind}.fasta

        # 4. align
        {params.BWA} mem -t {threads} {wildcards.dataset}/QA_alignments/bwa_refs_{wildcards.kind}.fasta {input.R1} {input.R2} > {output.SAM}

        # 5. MSA
        {params.MAFFT} --nuc --preservecase --maxiterate 1000 --localpair --thread {threads} {wildcards.dataset}/QA_alignments/bwa_refs_{wildcards.kind}.fasta > {output.MSA}

        # 6. cleanup BWA indices
        rm -f {wildcards.dataset}/QA_alignments/bwa_refs_{wildcards.kind}.fasta.*
        """

# 4b. Call coverage statistics
rule coverage_QA:
    input:
        BAM = "{dataset}/QA_alignments/bwa_aln_{kind}.bam",
        MSA = "{dataset}/QA_alignments/bwa_refs_msa_{kind}.fasta",
    output:
        "{dataset}/QA_alignments/coverage_{kind}.tsv",
    params:
        lsfoutfile = "{dataset}/QA_alignments/coverage_QA_{kind}.lsfout.log",
        lsferrfile = "{dataset}/QA_alignments/coverage_QA_{kind}.lsferr.log",
        scratch = '1250',
        mem = config.coverage_QA['mem'],
        time = config.coverage_QA['time'],
        COV_STATS = config.applications['coverage_stats'],
    conda:
        config.coverage_QA['conda']
    benchmark:
        "{dataset}/QA_alignments/coverage_QA_{kind}.benchmark"
    threads:
        1
    shell:
        """
        CONSENSUS_NAME={wildcards.dataset}
        CONSENSUS_NAME="${{CONSENSUS_NAME#*/}}"
        CONSENSUS_NAME="${{CONSENSUS_NAME//\//-}}"

        # 1. clean previous run
        rm -f {output}
        mkdir -p {wildcards.dataset}/QA_alignments

        # 2. collect coverage stats
        # we only collect statistics in the loop regions
        # of HIV-1 in order
        {params.COV_STATS} -t HXB2:6614-6812,7109-7217,7376-7478,7601-7634 -i {input.BAM} -o {output} -m {input.MSA} --select "${{CONSENSUS_NAME}}"
        """


# 5. construct MSA from all patient files
def construct_msa_input_files(wildcards):
    output_list = ["{}{}.fasta".format(s, wildcards.kind)
                   for s in references]
    output_list.append("references/HXB2.fasta")

    return output_list


rule msa:
    input:
        construct_msa_input_files
    output:
        "references/ALL_aln_{kind}.fasta"
    params:
        lsfoutfile = "references/MAFFT_{kind}_cohort.lsfout.log",
        lsferrfile = "references/MAFFT_{kind}_cohort.lsferr.log",
        scratch = '1250',
        mem = config.msa['mem'],
        time = config.msa['time'],
        MAFFT = config.applications['mafft'],
    conda:
        config.msa['conda']
    benchmark:
        "references/MAFFT_{kind}_cohort.benchmark"
    threads:
        config.msa['threads']
    shell:
        """
        cat {input} > ALL_{wildcards.kind}.fasta
        {params.MAFFT} --nuc --preservecase --maxiterate 1000 --localpair --thread {threads} ALL_{wildcards.kind}.fasta > {output}
        rm ALL_{wildcards.kind}.fasta
        """

rule msaclean:
    shell:
        """
        rm -rf references/ALL_aln_*.fasta
        rm -rf references/MAFFT_*_cohort.*
        """


# 6. convert alignments to HXB2 alignment
rule convert_to_hxb2:
    input:
        REF_ambig = "references/ALL_aln_ambig.fasta",
        REF_majority = "references/ALL_aln_majority.fasta",
        BAM = "{dataset}/alignments/full_aln.bam",
        REJECTS_BAM = "{dataset}/alignments/rejects.bam",
    output:
        "{dataset}/alignments/HXB2_aln.bam"
    params:
        lsfoutfile = "{dataset}/alignments/convert_to_HXB2.lsfout.log",
        lsferrfile = "{dataset}/alignments/convert_to_HXB2.lsferr.log",
        scratch = '1250',
        mem = config.convert_to_hxb2['mem'],
        time = config.convert_to_hxb2['time'],
        CONVERT_REFERENCE = config.applications['convert_reference'],
    conda:
        config.convert_to_hxb2['conda']
    benchmark:
        "{dataset}/alignments/convert_to_HXB2.benchmark"
    threads:
        1
    shadow:
        "shallow"
    shell:
        """
        {params.CONVERT_REFERENCE} -t HXB2 -m {input.REF_ambig} -i {input.BAM} -o {output}
        """


rule alignclean:
    shell:
        """
        rm -rf patients/*/*/alignments
        rm -rf patients/*/*/QA_alignments
        rm -rf patients/*/*/references/ref_ambig.fasta
        rm -rf patients/*/*/references/ref_majority.fasta
        """
